{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ClassificationLayer(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.ln1 = nn.Linear(input_dim,input_dim*2)\n",
    "        self.ln2 = nn.Linear(input_dim*2,output_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.drop =nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "\n",
    "        :param x: (N,L,D)\n",
    "        :return: prob. distribution (N,L,dout)\n",
    "        '''\n",
    "        x = self.ln1(x) # (N,L,D*2)\n",
    "        x = self.act(x)\n",
    "        logits = self.ln2(x) # (N,L,dout)\n",
    "        logits = self.drop(logits)\n",
    "        probs = nn.Softmax(-1)(logits)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.1482, 0.3409, 0.7603,  ..., 0.5581, 0.6056, 0.0717],\n         [0.8745, 0.2910, 0.6281,  ..., 0.3196, 0.6573, 0.2279],\n         [0.5815, 0.7399, 0.7406,  ..., 0.9813, 0.5961, 0.7176],\n         ...,\n         [0.9861, 0.0863, 0.4987,  ..., 0.8114, 0.2574, 0.0361],\n         [0.8659, 0.9893, 0.1339,  ..., 0.8401, 0.3677, 0.4428],\n         [0.4787, 0.5477, 0.8247,  ..., 0.9343, 0.7346, 0.1771]],\n\n        [[0.3019, 0.8349, 0.9631,  ..., 0.8688, 0.6517, 0.3198],\n         [0.9515, 0.3885, 0.1247,  ..., 0.7233, 0.2207, 0.6691],\n         [0.6933, 0.1910, 0.7373,  ..., 0.3218, 0.2361, 0.0115],\n         ...,\n         [0.6495, 0.9091, 0.0282,  ..., 0.5070, 0.8919, 0.5179],\n         [0.0026, 0.2610, 0.5566,  ..., 0.6671, 0.1637, 0.9657],\n         [0.1130, 0.6909, 0.4184,  ..., 0.5202, 0.2963, 0.6070]]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ClassificationLayer(768,4000)\n",
    "x = torch.rand(2,30,768)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 30, 4000]),\n tensor([[[0.0003, 0.0003, 0.0002,  ..., 0.0003, 0.0002, 0.0002],\n          [0.0002, 0.0002, 0.0003,  ..., 0.0002, 0.0002, 0.0002],\n          [0.0002, 0.0003, 0.0002,  ..., 0.0003, 0.0002, 0.0002],\n          ...,\n          [0.0002, 0.0002, 0.0003,  ..., 0.0002, 0.0002, 0.0002],\n          [0.0003, 0.0003, 0.0002,  ..., 0.0003, 0.0002, 0.0002],\n          [0.0002, 0.0003, 0.0003,  ..., 0.0002, 0.0003, 0.0002]],\n \n         [[0.0002, 0.0003, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n          [0.0003, 0.0002, 0.0003,  ..., 0.0002, 0.0002, 0.0002],\n          [0.0003, 0.0003, 0.0003,  ..., 0.0002, 0.0002, 0.0002],\n          ...,\n          [0.0002, 0.0002, 0.0003,  ..., 0.0002, 0.0002, 0.0002],\n          [0.0002, 0.0003, 0.0003,  ..., 0.0003, 0.0002, 0.0002],\n          [0.0002, 0.0003, 0.0002,  ..., 0.0002, 0.0002, 0.0002]]],\n        grad_fn=<SoftmaxBackward0>))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = clf(x)\n",
    "out.shape, out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}