{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class NaiveEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.dim = hidden_dim\n",
    "        self.Wq = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wk = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wv = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.layerNorm_SA = nn.LayerNorm(self.dim)\n",
    "\n",
    "        self.ffn1 = nn.Linear(self.dim,self.dim*4)\n",
    "        self.ffn2 = nn.Linear(self.dim*4,self.dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.layerNorm_ffn = nn.LayerNorm(self.dim)\n",
    "\n",
    "    def SelfAttention(self, x):\n",
    "        '''\n",
    "\n",
    "        :param x: (N,L,D)\n",
    "        :return: (N,L,D)\n",
    "        '''\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "\n",
    "        attention_score = torch.matmul(Q,K.transpose(1,2))/math.sqrt(self.dim)\n",
    "        attention_score = nn.Softmax(-1)(attention_score)\n",
    "        O = torch.matmul(attention_score,V)\n",
    "        O = self.layerNorm_SA(x + O)\n",
    "        return O\n",
    "\n",
    "    def FFN(self,x):\n",
    "        tmp1 = self.act(self.ffn1(x))\n",
    "        tmp2 = self.ffn2(tmp1)\n",
    "        output = self.layerNorm_ffn(x+tmp2)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "\n",
    "        :param x: shape (N,L,D) N is batch size, L is the length of the sequnce, D is the dimension of word embeddings\n",
    "        :return: shape (N,L,D)\n",
    "        '''\n",
    "        x = self.SelfAttention(x)\n",
    "        x = self.FFN(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 缺点\n",
    "1. 没有dropout\n",
    "2. 没有multi-head attention\n",
    "3. 没有attention mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.0401, -0.6295, -0.3225,  ..., -0.8235,  1.5486,  0.7692],\n         [-0.1269,  1.3418, -0.3858,  ...,  0.6867, -0.3089,  0.0166],\n         [-0.2949,  1.0402,  0.1966,  ...,  0.7793, -1.5150,  0.3857],\n         ...,\n         [-0.6879, -1.0686,  1.2191,  ...,  0.0429, -0.9116, -1.0797],\n         [ 0.8545, -0.9302,  0.1264,  ...,  0.0619, -2.1369, -0.5130],\n         [ 0.0893, -1.1508,  0.3735,  ...,  2.2304,  0.2492, -1.0066]],\n\n        [[-0.0602,  0.8188,  0.8021,  ..., -0.8600,  0.2093,  1.8612],\n         [ 1.8572,  0.9054, -1.1331,  ..., -0.9275, -1.4083,  2.2626],\n         [ 0.1644,  0.5029,  1.3739,  ..., -0.4150,  0.0186, -0.1931],\n         ...,\n         [-0.4816, -0.1627,  0.5056,  ...,  0.7688,  0.4624, -1.2395],\n         [-0.8423, -1.4215,  0.3208,  ..., -0.2339,  1.0953, -1.2284],\n         [ 2.1312, -0.8551,  0.0459,  ..., -0.6376,  2.3748, -0.0283]],\n\n        [[ 0.9800,  0.1088,  0.3221,  ..., -0.3778,  0.5116,  0.0306],\n         [-2.0665, -0.2299,  0.0962,  ..., -0.2691,  0.8272,  0.7985],\n         [-1.0420, -0.3688,  0.9299,  ...,  1.2991,  1.1964,  0.3424],\n         ...,\n         [-0.6240,  0.9708,  0.2504,  ...,  2.2319,  1.6388, -1.2759],\n         [ 0.0676, -0.3959, -0.2256,  ...,  1.2893,  0.5033, -0.1445],\n         [-0.8306,  0.0334,  2.3923,  ...,  1.6829,  0.4373,  1.1324]],\n\n        ...,\n\n        [[-0.3811,  2.8330,  0.1229,  ..., -1.5791, -0.6332,  0.8478],\n         [ 1.0992,  0.4911, -0.5664,  ...,  0.2063, -1.0188, -1.5192],\n         [-0.2233,  1.6546, -0.1613,  ...,  0.7402, -1.6859, -1.8189],\n         ...,\n         [-0.4643,  0.5897,  0.6164,  ..., -0.2338,  1.3525, -0.4600],\n         [-0.2431,  0.5471,  1.4960,  ..., -3.4062, -0.0441, -2.4438],\n         [-0.4670, -0.8150, -1.4390,  ..., -0.5887,  0.8781,  0.5290]],\n\n        [[ 0.9659,  1.6661, -0.5424,  ...,  1.3086, -0.1677, -1.3544],\n         [ 0.0544, -1.4426, -0.2040,  ..., -1.7507,  0.6435,  0.1586],\n         [-0.9068, -0.0443,  0.0817,  ...,  0.4886, -0.3761,  0.6434],\n         ...,\n         [ 0.0862,  0.1788,  1.5466,  ...,  0.9911,  0.8244, -1.7994],\n         [-0.2233,  0.0157,  0.1865,  ...,  0.2427, -0.5978,  1.6473],\n         [ 0.2789,  0.2458, -1.1483,  ..., -0.1007,  0.3963, -1.0258]],\n\n        [[-0.6379,  0.5594, -0.2602,  ...,  0.1368,  0.0288,  1.0787],\n         [-1.6899,  0.8009, -1.7486,  ..., -0.5176,  0.4040,  1.5693],\n         [-1.2862, -1.0288, -0.2544,  ..., -0.8255, -0.4725,  0.1803],\n         ...,\n         [-1.2684, -2.0096,  1.1884,  ...,  1.0553, -1.0970, -0.2228],\n         [ 0.9837, -0.5552, -2.5289,  ...,  1.4613,  0.3440, -0.0975],\n         [ 0.4026, -2.2418, -0.1994,  ...,  0.5969, -0.9856,  0.8843]]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.randn(10,50,200)\n",
    "X = torch.Tensor(X)\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "naive_encoder = NaiveEncoderLayer(200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([10, 50, 200]),\n tensor([[[-9.1078e-01, -9.4485e-01, -1.6519e-01,  ..., -8.1986e-01,\n            1.3281e+00,  7.8501e-01],\n          [-7.0449e-02,  9.2821e-01, -4.7323e-01,  ...,  8.5309e-01,\n           -3.1893e-01, -1.6464e-01],\n          [-4.4850e-02,  1.2840e+00,  2.8053e-02,  ...,  6.2792e-01,\n           -1.1707e+00,  4.1003e-01],\n          ...,\n          [-7.9872e-01, -6.7767e-01,  1.1246e+00,  ..., -3.9275e-01,\n           -6.8273e-01, -1.1737e+00],\n          [ 1.0243e+00, -7.3214e-01, -2.5325e-02,  ...,  5.3646e-02,\n           -2.0490e+00, -5.1975e-01],\n          [ 4.9983e-02, -1.4734e+00,  4.7782e-01,  ...,  2.1851e+00,\n            3.6147e-01, -1.0625e+00]],\n \n         [[ 1.1065e-01,  7.1265e-01,  8.9405e-01,  ..., -7.4141e-01,\n            2.1874e-01,  1.7257e+00],\n          [ 1.5766e+00,  7.6594e-01, -1.3727e+00,  ..., -8.1249e-01,\n           -1.2478e+00,  2.0054e+00],\n          [ 1.5012e-01,  6.9972e-02,  1.3647e+00,  ..., -6.2881e-01,\n            2.2019e-01,  2.2047e-01],\n          ...,\n          [-4.0242e-01, -2.1396e-01,  4.5234e-01,  ...,  7.8153e-01,\n            6.4068e-01, -9.4703e-01],\n          [-4.0980e-01, -1.5239e+00,  4.4662e-01,  ..., -2.5696e-01,\n            1.2036e+00, -7.7715e-01],\n          [ 2.2785e+00, -7.9999e-01,  1.5166e-01,  ..., -1.9655e-02,\n            2.2813e+00,  2.6580e-03]],\n \n         [[ 1.0576e+00,  1.0711e-01,  6.2789e-01,  ..., -2.1325e-01,\n            6.5433e-01,  3.7135e-01],\n          [-1.5778e+00,  1.8967e-01,  5.1811e-01,  ..., -1.5274e-01,\n            9.3302e-01,  9.6093e-01],\n          [-5.5580e-01, -3.3554e-01,  1.1198e+00,  ...,  1.2501e+00,\n            1.2645e+00,  8.4347e-01],\n          ...,\n          [-6.1742e-01,  8.5826e-01,  4.6448e-01,  ...,  1.7049e+00,\n            1.3663e+00, -1.2597e+00],\n          [ 6.8451e-02, -3.7634e-01, -8.8751e-02,  ...,  1.4602e+00,\n            3.2990e-01, -2.8311e-02],\n          [-7.1017e-01, -6.5765e-02,  2.2099e+00,  ...,  1.6878e+00,\n            6.2975e-01,  9.3130e-01]],\n \n         ...,\n \n         [[-3.5730e-01,  2.8437e+00,  4.1028e-02,  ..., -1.6282e+00,\n           -7.2705e-01,  1.2525e+00],\n          [ 1.2660e+00,  6.3727e-01, -3.3939e-01,  ..., -1.7365e-01,\n           -1.2161e+00, -1.2935e+00],\n          [-3.6272e-02,  1.7904e+00,  3.8274e-02,  ...,  7.9786e-01,\n           -1.4629e+00, -1.4387e+00],\n          ...,\n          [-3.2582e-01,  1.0659e+00,  7.6578e-01,  ..., -2.4173e-01,\n            1.4636e+00, -4.3491e-01],\n          [-2.2281e-01,  7.8303e-01,  1.4325e+00,  ..., -2.9077e+00,\n            1.0365e-02, -2.0422e+00],\n          [-5.7112e-01, -3.1577e-01, -1.1652e+00,  ..., -4.8044e-01,\n            1.2312e+00,  7.7979e-01]],\n \n         [[ 1.5498e+00,  1.5550e+00, -9.8082e-01,  ...,  1.3669e+00,\n           -2.0504e-01, -1.3284e+00],\n          [ 3.2699e-01, -1.7976e+00,  1.0101e-01,  ..., -1.5724e+00,\n            1.0872e+00,  6.0195e-01],\n          [-5.5919e-01, -3.6695e-01,  5.0688e-03,  ...,  9.4549e-01,\n           -4.1388e-01,  9.5608e-01],\n          ...,\n          [ 2.6803e-01,  1.2934e-01,  1.3907e+00,  ...,  1.0403e+00,\n            7.1452e-01, -1.6296e+00],\n          [-2.4874e-01, -1.8422e-01, -1.7327e-01,  ...,  3.4254e-02,\n           -4.3132e-01,  1.8139e+00],\n          [ 6.8748e-01, -7.0375e-02, -1.2210e+00,  ...,  9.3629e-03,\n            9.5972e-01, -6.7547e-01]],\n \n         [[-4.3090e-01,  6.1857e-01, -4.5925e-02,  ...,  2.0496e-01,\n            2.9001e-02,  9.0141e-01],\n          [-1.4043e+00,  1.0137e+00, -1.8709e+00,  ..., -4.9024e-01,\n            2.7701e-01,  1.4644e+00],\n          [-1.1845e+00, -1.1145e+00, -4.3437e-01,  ..., -6.2662e-01,\n           -4.1684e-01,  3.4693e-01],\n          ...,\n          [-1.2489e+00, -2.0698e+00,  8.3315e-01,  ...,  1.1524e+00,\n           -1.2619e+00, -3.2618e-01],\n          [ 5.2064e-01, -7.3263e-01, -2.8876e+00,  ...,  9.4763e-01,\n            2.2320e-01, -5.8551e-02],\n          [ 1.9363e-01, -2.2309e+00, -3.3486e-01,  ...,  6.5010e-01,\n           -7.3533e-01,  8.8703e-01]]], grad_fn=<NativeLayerNormBackward0>))"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = naive_encoder(X)\n",
    "\n",
    "output.shape,output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}