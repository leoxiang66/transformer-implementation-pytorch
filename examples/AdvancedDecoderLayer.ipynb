{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.dim = hidden_dim\n",
    "\n",
    "        # multi-head SA\n",
    "        self.num_heads = 12\n",
    "        self.dim_per_head = self.dim // self.num_heads\n",
    "        self.Wq = nn.Linear(self.dim, self.num_heads * self.dim_per_head, bias=False)\n",
    "        self.Wk = nn.Linear(self.dim, self.num_heads * self.dim_per_head, bias=False)\n",
    "        self.Wv = nn.Linear(self.dim, self.num_heads * self.dim_per_head, bias=False)\n",
    "        self.layerNorm_SA = nn.LayerNorm(self.dim)\n",
    "        self.W_reshape_back_SA = nn.Linear(self.num_heads * self.dim_per_head, self.dim)\n",
    "\n",
    "        # Multi-head CA\n",
    "        self.Wq2 = nn.Linear(self.dim, self.num_heads * self.dim_per_head, bias=False)\n",
    "        self.Wk2 = nn.Linear(self.dim, self.num_heads * self.dim_per_head, bias=False)\n",
    "        self.Wv2 = nn.Linear(self.dim, self.num_heads * self.dim_per_head, bias=False)\n",
    "        self.layerNorm_CA = nn.LayerNorm(self.dim)\n",
    "        self.W_reshape_back_CA = nn.Linear(self.num_heads * self.dim_per_head, self.dim)\n",
    "\n",
    "\n",
    "        # FFN\n",
    "        self.ffn1 = nn.Linear(self.dim,self.dim*4)\n",
    "        self.ffn2 = nn.Linear(self.dim*4,self.dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.layerNorm_ffn = nn.LayerNorm(self.dim)\n",
    "\n",
    "        # dropout\n",
    "        self.att_drop_prob = 0.1\n",
    "        self.state_drop_prob = 0.5\n",
    "        self.att_drop = nn.Dropout(self.att_drop_prob)\n",
    "        self.state_drop = nn.Dropout(self.state_drop_prob)\n",
    "\n",
    "\n",
    "\n",
    "    def _compute_SA_pad_mask(self, attention_mask):\n",
    "        '''\n",
    "\n",
    "        :param attention_mask:  (N,L1)\n",
    "        :return: (N,#heads, L1,L1)\n",
    "        '''\n",
    "        mask = torch.zeros((attention_mask.size(0), self.num_heads, attention_mask.size(1),\n",
    "                                  attention_mask.size(1)),dtype=torch.int32)\n",
    "        mask = mask + attention_mask[:, None, None, :]\n",
    "        return mask\n",
    "\n",
    "    def _compute_att_subsequence_mask(self, x):\n",
    "        '''\n",
    "\n",
    "        :param x: (N,L1,D)\n",
    "        :return: (N,#heads, L1,L1)\n",
    "        '''\n",
    "        mask = torch.zeros((x.size(0),self.num_heads,x.size(1),x.size(1)),dtype=torch.int32)\n",
    "        ones = torch.tril(torch.ones((x.size(1), x.size(1)),dtype=torch.int32), diagonal=0)\n",
    "        mask += ones\n",
    "        return mask\n",
    "\n",
    "    def _compute_SA_mask_logits(self, x, attention_mask):\n",
    "        '''\n",
    "\n",
    "        :param x: (N,L1,D)\n",
    "        :param attention_mask: (N,L1)\n",
    "        :return: (N,#heads, L1,L1)\n",
    "        '''\n",
    "        att_pad_mask = self._compute_SA_pad_mask(attention_mask)\n",
    "        att_subseq_mask = self._compute_att_subsequence_mask(x)\n",
    "        mask = att_pad_mask & att_subseq_mask\n",
    "        mask_logits = (1.0 - mask) * -10000.0\n",
    "        return mask_logits\n",
    "\n",
    "    def _compute_CA_pad_mask(self,x,enc_att_mask):\n",
    "        '''\n",
    "\n",
    "        :param x: decoder input: (N,L1,D)\n",
    "        :param enc_att_mask: (N,L2)\n",
    "        :return: (N,#heads, L1,L2)\n",
    "        '''\n",
    "        mask = torch.zeros((x.size(0), self.num_heads, x.size(1),\n",
    "                            enc_att_mask.size(1)), dtype=torch.int32)\n",
    "        mask = mask + enc_att_mask[:, None, None, :]\n",
    "        return mask\n",
    "\n",
    "    def _compute_CA_mask_logits(self,x,enc_att_mask):\n",
    "        '''\n",
    "\n",
    "        :param x: decoder input: (N,L1,D)\n",
    "        :param enc_att_mask: (N,L2)\n",
    "        :return: (N,#heads, L1,L2)\n",
    "        '''\n",
    "        mask = self._compute_CA_pad_mask(x,enc_att_mask)\n",
    "        mask_logits = (1.0 - mask) * -10000.0\n",
    "        return mask_logits\n",
    "\n",
    "\n",
    "    def MultiHeadSelfAttention(self, x,attention_mask):\n",
    "        '''\n",
    "\n",
    "        :param x: (N,L1,D)\n",
    "        :return: (N,L1,D)\n",
    "        '''\n",
    "        '''\n",
    "        Q,K,V:\n",
    "\n",
    "        (N,L,(#heads * dph)) ->(N,#heads,L,dph)\n",
    "        '''\n",
    "        new_size = x.size()[:-1] + (self.num_heads, self.dim_per_head)  # (N,L1, #heads, dph)\n",
    "        Q = self.Wq(x).view(*new_size).permute(0, 2, 1, 3)  # (N,#heads,L1,dph)\n",
    "        K = self.Wk(x).view(*new_size).permute(0, 2, 1, 3)\n",
    "        V = self.Wv(x).view(*new_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        attention_score = torch.matmul(Q,K.transpose(-2,-1))/math.sqrt(self.dim) # (N,#heads,L1, L1)\n",
    "        attention_score += self._compute_SA_mask_logits(x,attention_mask)\n",
    "        attention_score = nn.Softmax(-1)(attention_score)\n",
    "        attention_score = self.att_drop(attention_score)\n",
    "        O = torch.matmul(attention_score,V)\n",
    "        O = O.permute(0, 2, 1, 3)  # (N,L1, #heads, dph)\n",
    "        O = O.contiguous().view(x.size(0), x.size(1), -1)  # (N,L1, #heads*dph)\n",
    "        O = self.W_reshape_back_SA(O)  # (N,L1,D)\n",
    "        O = self.state_drop(O)\n",
    "        O = self.layerNorm_SA(x + O)\n",
    "        return O\n",
    "\n",
    "\n",
    "    def MultiHeadCrossAttention(self, x1, x2,enc_att_mask):\n",
    "        '''\n",
    "\n",
    "        :param x1: decoder input: (N,L1,D)\n",
    "        :param x2: encoder output: (N,L2,D)\n",
    "        :return: (N,L1,D)\n",
    "        '''\n",
    "        '''\n",
    "        Q,K,V:\n",
    "\n",
    "        (N,L,(#heads * dph)) ->(N,#heads,L,dph)\n",
    "        '''\n",
    "        N = x1.size(0)\n",
    "        Q = self.Wq2(x1).view(N, -1, self.num_heads, self.dim_per_head).transpose(1, 2)  # Q: [N, n_heads, L1, dph]\n",
    "        K = self.Wk2(x2).view(N, -1, self.num_heads, self.dim_per_head).transpose(1, 2)  # K: [N, n_heads, L2, dph]\n",
    "        V = self.Wv2(x2).view(N, -1, self.num_heads, self.dim_per_head).transpose(1, 2)  # V: [N, n_heads, L2, dph]\n",
    "\n",
    "        attention_score = torch.matmul(Q,K.transpose(-2,-1))/math.sqrt(self.dim)    #[N, n_heads, L1, L2]\n",
    "        attention_score += self._compute_CA_mask_logits(x1,enc_att_mask)\n",
    "        attention_score = nn.Softmax(-1)(attention_score)\n",
    "        attention_score = self.att_drop(attention_score)\n",
    "        O = torch.matmul(attention_score,V) # [N, n_heads, L1, dph]\n",
    "        O = O.permute(0, 2, 1, 3)  # (N,L1, n_heads, dph)\n",
    "        O = O.contiguous().view(N, x1.size(1), -1)  # (N,L1, n_heads*dph)\n",
    "        O = self.W_reshape_back_CA(O)  # (N,L1,D)\n",
    "        O = self.layerNorm_SA(x1 + O)\n",
    "        return O\n",
    "\n",
    "\n",
    "    def FFN(self,x):\n",
    "        tmp1 = self.act(self.ffn1(x))\n",
    "        tmp2 = self.ffn2(tmp1)\n",
    "        tmp2 = self.state_drop(tmp2)\n",
    "        output = self.layerNorm_ffn(x+tmp2)\n",
    "        return output\n",
    "\n",
    "    def forward(self,x1,x2,dec_att_mask,enc_att_mask):\n",
    "        '''\n",
    "\n",
    "        :param x1: decoder input: (N,L1,D)\n",
    "        :param x2: encoder output: (N,L2,D)\n",
    "        :param dec_att_mask: (N,L1)\n",
    "        :param enc_att_mask: (N,L2)\n",
    "        :return:   (N,L1,D)\n",
    "        '''\n",
    "\n",
    "        x1 = self.MultiHeadSelfAttention(x1,dec_att_mask)\n",
    "        tmp = self.MultiHeadCrossAttention(x1, x2,enc_att_mask)\n",
    "        output = self.FFN(tmp)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0347,  2.0425, -0.2685,  ..., -1.8946, -0.0547, -0.0576],\n",
      "         [ 0.7092,  2.0814, -0.6385,  ..., -1.4035,  0.4594, -1.4810]],\n",
      "\n",
      "        [[ 0.9878,  0.2651, -0.0735,  ..., -2.1452,  0.4137, -0.0737],\n",
      "         [ 0.4149,  0.0423, -0.7192,  ..., -0.1651,  1.1540,  0.0324]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>) torch.Size([2, 2, 768])\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = DecoderLayer(768)\n",
    "x1 = torch.ones(2, 2, 768)\n",
    "x2 = torch.ones(2, 3, 768)\n",
    "\n",
    "enc_att_mask = torch.tensor([[1,1,0],[1,1,1]])\n",
    "dec_att_mask = torch.tensor([[1,1],[1,0]])\n",
    "\n",
    "output = decoder_layer(x1,x2,dec_att_mask,enc_att_mask)\n",
    "print(output, output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}