{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class NaiveEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.dim = hidden_dim\n",
    "        self.Wq = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wk = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wv = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.layerNorm_SA = nn.LayerNorm(self.dim)\n",
    "\n",
    "        self.ffn1 = nn.Linear(self.dim,self.dim*4)\n",
    "        self.ffn2 = nn.Linear(self.dim*4,self.dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.layerNorm_ffn = nn.LayerNorm(self.dim)\n",
    "\n",
    "    def SelfAttention(self, x):\n",
    "        '''\n",
    "\n",
    "        :param x: (N,L,D)\n",
    "        :return: (N,L,D)\n",
    "        '''\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "\n",
    "        attention_score = torch.matmul(Q,K.transpose(1,2))/math.sqrt(self.dim)\n",
    "        attention_score = nn.Softmax(-1)(attention_score)\n",
    "        O = torch.matmul(attention_score,V)\n",
    "        O = self.layerNorm_SA(x + O)\n",
    "        return O\n",
    "\n",
    "    def FFN(self,x):\n",
    "        tmp1 = self.act(self.ffn1(x))\n",
    "        tmp2 = self.ffn2(tmp1)\n",
    "        output = self.layerNorm_ffn(x+tmp2)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "\n",
    "        :param x: shape (N,L,D) N is batch size, L is the length of the sequnce, D is the dimension of word embeddings\n",
    "        :return: shape (N,L,D)\n",
    "        '''\n",
    "        x = self.SelfAttention(x)\n",
    "        x = self.FFN(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 缺点\n",
    "1. 没有dropout\n",
    "2. 没有multi-head attention\n",
    "3. 没有attention mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.3159, -0.5830,  1.7410,  ...,  0.3811,  0.7921,  1.0346],\n         [-0.5891, -0.5257, -0.2432,  ..., -1.1997,  0.5752, -1.3614],\n         [ 0.2590,  2.6640, -1.0004,  ...,  0.5402,  1.5058,  0.6942],\n         ...,\n         [ 1.0217,  1.7838, -0.6132,  ...,  0.8607, -0.1479, -0.7465],\n         [ 0.6748,  0.5542,  1.2293,  ...,  0.7653, -0.4950, -1.9648],\n         [ 0.5786,  0.3714,  1.4466,  ..., -0.5294, -0.4966, -1.0818]],\n\n        [[-2.9085,  0.5468, -0.6978,  ...,  0.1342, -0.9079,  2.0059],\n         [-0.9772, -0.7716,  1.6000,  ...,  2.1822, -0.7635,  1.2609],\n         [ 0.0752,  1.0175, -0.7128,  ...,  0.6540,  0.5588,  1.5919],\n         ...,\n         [-2.3571, -0.8133, -1.5915,  ...,  2.4165, -1.3593,  1.6513],\n         [-0.4299, -0.4517, -0.5844,  ..., -0.7948,  0.1466,  0.2284],\n         [-0.3546,  1.4316,  1.3139,  ..., -0.3075, -0.7003,  1.6643]],\n\n        [[-1.5730, -0.5587,  1.5377,  ...,  0.0732, -0.7021,  0.5502],\n         [ 0.7607, -0.1438,  1.5209,  ..., -0.6326, -0.0729, -0.5736],\n         [-1.2554, -0.7902,  0.2360,  ...,  1.0289,  0.5021, -1.1407],\n         ...,\n         [ 0.6291, -0.3453,  0.9473,  ..., -0.2350,  1.1401,  0.6200],\n         [ 1.3326, -0.3710,  0.6156,  ...,  0.3326,  0.8174, -0.2766],\n         [-0.5748, -0.2887, -0.2206,  ...,  0.2672,  0.6624, -0.7322]],\n\n        ...,\n\n        [[ 1.1193, -0.9495, -2.9578,  ..., -0.1382,  0.3615,  0.4232],\n         [ 0.3742, -0.4605, -1.0946,  ..., -0.2080,  0.7417,  0.3148],\n         [ 0.1644,  1.1016,  0.5587,  ..., -1.4541, -0.0432,  0.9594],\n         ...,\n         [-0.2617, -1.4044,  0.6346,  ...,  0.7810, -0.2361, -0.0063],\n         [ 0.3393, -1.3932,  0.4851,  ..., -0.4291, -1.4396,  0.1343],\n         [ 0.4701, -0.0987, -0.2963,  ..., -0.9036,  0.3531, -1.1699]],\n\n        [[ 1.3675, -0.2687,  1.4439,  ..., -0.5558, -1.0908,  0.0105],\n         [-0.0264, -0.0506,  0.2352,  ..., -0.1931,  1.5084, -0.6821],\n         [ 0.0677, -0.2649,  0.4367,  ..., -0.9831, -0.4020, -1.9574],\n         ...,\n         [-0.0079,  0.8754,  0.5807,  ...,  1.0626, -0.0084,  0.7874],\n         [-0.1448, -0.1122,  0.2107,  ...,  0.2523,  1.1217,  0.1178],\n         [ 0.0186,  1.2430,  0.1093,  ...,  0.4855,  0.2851,  0.1367]],\n\n        [[-1.6507,  0.0494,  0.2399,  ..., -0.6824, -0.0355,  0.3860],\n         [ 1.0834, -0.1937, -0.4674,  ...,  0.0131, -1.2173,  1.3501],\n         [ 1.3963,  1.3238, -1.1620,  ...,  0.3078,  0.5736, -0.6859],\n         ...,\n         [-0.0507, -0.8755, -0.5422,  ..., -2.3249, -0.6863,  0.4294],\n         [ 1.0268, -0.6232,  2.3787,  ..., -0.9321, -2.1463,  0.4526],\n         [ 2.2247, -0.0981, -0.5668,  ..., -0.2750,  0.0403,  0.1985]]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.randn(10,50,200)\n",
    "X = torch.Tensor(X)\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "naive_encoder = NaiveEncoder(200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([10, 50, 200]),\n tensor([[[-3.1560e-01, -3.4546e-01,  1.3904e+00,  ...,  2.3509e-01,\n            5.5878e-01,  1.0912e+00],\n          [-1.9650e-01, -6.3841e-01, -2.0755e-01,  ..., -9.8208e-01,\n            6.4689e-01, -1.7955e+00],\n          [ 1.9568e-01,  2.4087e+00, -1.5424e+00,  ...,  5.4551e-01,\n            1.1766e+00,  5.3722e-01],\n          ...,\n          [ 9.9189e-01,  1.5846e+00, -3.9716e-01,  ...,  1.0545e+00,\n           -4.6478e-01, -8.6285e-01],\n          [ 7.6432e-01,  2.7416e-01,  1.5233e+00,  ...,  7.8932e-01,\n           -8.1846e-01, -1.9955e+00],\n          [ 6.3419e-01,  3.0797e-01,  1.5341e+00,  ..., -3.8921e-01,\n           -1.7943e-01, -1.3472e+00]],\n \n         [[-2.9542e+00,  7.0059e-01, -9.7643e-01,  ...,  8.5056e-02,\n           -9.4083e-01,  2.3823e+00],\n          [-9.8874e-01, -1.0092e+00,  1.0984e+00,  ...,  2.3523e+00,\n           -1.1913e+00,  1.0914e+00],\n          [-7.3326e-02,  7.9250e-01, -9.4479e-01,  ...,  7.0494e-01,\n            7.7909e-01,  1.8209e+00],\n          ...,\n          [-1.8072e+00, -2.5527e-01, -1.5393e+00,  ...,  2.3964e+00,\n           -1.0432e+00,  1.5961e+00],\n          [-7.0646e-01, -4.5819e-01, -7.0471e-01,  ..., -3.1560e-01,\n            2.7910e-01,  3.3450e-01],\n          [-2.3332e-01,  1.1171e+00,  1.0680e+00,  ...,  1.3839e-01,\n           -7.5247e-01,  1.5858e+00]],\n \n         [[-1.1797e+00, -3.6169e-01,  1.1206e+00,  ..., -8.4639e-02,\n           -3.8588e-01,  1.4416e-01],\n          [ 7.6090e-01, -1.4247e-01,  1.4300e+00,  ..., -4.7638e-01,\n           -1.1615e-02, -4.0314e-01],\n          [-1.0433e+00, -7.3693e-01,  2.7125e-02,  ...,  1.2838e+00,\n            6.9592e-01, -8.9135e-01],\n          ...,\n          [ 2.1691e-01, -4.4945e-01,  5.8774e-01,  ..., -6.5252e-01,\n            1.1860e+00,  6.6149e-01],\n          [ 1.2855e+00, -5.6585e-01,  7.4378e-01,  ...,  4.5600e-01,\n            6.2862e-01, -4.1091e-01],\n          [-5.5202e-01, -4.9789e-01, -5.6691e-01,  ...,  2.7075e-01,\n            6.3071e-01, -6.6323e-01]],\n \n         ...,\n \n         [[ 8.4109e-01, -8.9805e-01, -3.0021e+00,  ..., -1.2437e-01,\n            8.3258e-01,  5.9398e-01],\n          [ 1.8990e-01, -6.0060e-01, -1.1706e+00,  ..., -4.1431e-01,\n            1.0082e+00,  1.9958e-01],\n          [-1.6324e-01,  1.0858e+00,  4.7434e-01,  ..., -1.4967e+00,\n            4.5397e-02,  1.0967e+00],\n          ...,\n          [-1.8800e-01, -1.0443e+00,  5.3181e-01,  ...,  7.3332e-01,\n           -8.1684e-02,  2.9199e-01],\n          [ 4.1210e-01, -1.0463e+00,  4.8808e-01,  ..., -5.3790e-01,\n           -1.1418e+00,  3.0848e-01],\n          [ 3.3575e-01,  9.1168e-02, -4.3767e-01,  ..., -1.1619e+00,\n            4.4869e-01, -8.6779e-01]],\n \n         [[ 1.7621e+00,  1.6625e-01,  1.5018e+00,  ..., -2.2089e-01,\n           -1.1492e+00, -3.2575e-01],\n          [ 5.7562e-01,  1.5616e-03,  3.7231e-01,  ...,  2.4520e-01,\n            1.6949e+00, -7.5875e-01],\n          [ 1.9724e-01, -1.3207e-01,  1.9483e-01,  ..., -9.8695e-01,\n           -8.0496e-01, -2.1651e+00],\n          ...,\n          [-1.0274e-01,  8.0791e-01,  2.9715e-01,  ...,  1.2030e+00,\n           -2.1229e-01,  4.3864e-01],\n          [-1.8591e-01,  1.5465e-02,  2.9034e-01,  ...,  4.1392e-01,\n            1.0319e+00, -1.1189e-01],\n          [-3.0309e-01,  1.3497e+00,  2.3517e-02,  ...,  5.2802e-01,\n            2.1044e-01, -1.9679e-01]],\n \n         [[-1.4611e+00, -2.9361e-01, -2.1459e-01,  ..., -6.1177e-01,\n           -2.3683e-01,  6.3013e-01],\n          [ 8.0574e-01, -3.4144e-01, -8.1709e-01,  ..., -1.8552e-01,\n           -1.3187e+00,  1.3179e+00],\n          [ 1.5051e+00,  6.4181e-01, -1.2105e+00,  ...,  5.0512e-01,\n            2.0374e-01, -7.3678e-01],\n          ...,\n          [ 1.2281e-01, -5.9668e-01, -4.9157e-01,  ..., -2.2982e+00,\n           -1.2319e+00,  1.5663e-01],\n          [ 9.2078e-01, -6.4124e-01,  2.0728e+00,  ..., -9.8321e-01,\n           -2.3050e+00,  6.5875e-01],\n          [ 2.5055e+00, -4.2913e-01, -1.1094e+00,  ...,  4.5559e-02,\n           -2.5212e-01,  2.0176e-01]]], grad_fn=<NativeLayerNormBackward0>))"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = naive_encoder(X)\n",
    "\n",
    "output.shape,output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}