{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class NaiveDecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.dim = hidden_dim\n",
    "\n",
    "        # SA\n",
    "        self.Wq = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wk = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wv = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.layerNorm_SA = nn.LayerNorm(self.dim)\n",
    "\n",
    "        # CA\n",
    "        self.Wq2 = nn.Linear(self.dim,self.dim,bias=False)\n",
    "        self.Wk2 = nn.Linear(self.dim,self.dim,bias=False)\n",
    "        self.Wv2 = nn.Linear(self.dim,self.dim,bias=False)\n",
    "        self.layerNorm_CA = nn.LayerNorm(self.dim)\n",
    "\n",
    "\n",
    "        # FFN\n",
    "        self.ffn1 = nn.Linear(self.dim,self.dim*4)\n",
    "        self.ffn2 = nn.Linear(self.dim*4,self.dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.layerNorm_ffn = nn.LayerNorm(self.dim)\n",
    "\n",
    "    def SelfAttention(self, x):\n",
    "        '''\n",
    "\n",
    "        :param x: (N,L,D)\n",
    "        :return: (N,L,D)\n",
    "        '''\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "\n",
    "        attention_score = torch.matmul(Q,K.transpose(1,2))/math.sqrt(self.dim)\n",
    "        attention_score = nn.Softmax(-1)(attention_score)\n",
    "        O = torch.matmul(attention_score,V)\n",
    "        O = self.layerNorm_SA(x + O)\n",
    "        return O\n",
    "\n",
    "    def CrossAttention(self,x1,x2):\n",
    "        '''\n",
    "\n",
    "        :param x1: decoder input: (N,L,D)\n",
    "        :param x2: encoder output: (N,L,D)\n",
    "        :return: (N,L,D)\n",
    "        '''\n",
    "        Q = self.Wq2(x1)\n",
    "        K = self.Wk2(x2)\n",
    "        V = self.Wv2(x2)\n",
    "\n",
    "        attention_score = torch.matmul(Q,K.transpose(1,2))/math.sqrt(self.dim)\n",
    "        attention_score = nn.Softmax(-1)(attention_score)\n",
    "        O = torch.matmul(attention_score,V)\n",
    "        O = self.layerNorm_SA(x1 + O)\n",
    "        return O\n",
    "\n",
    "\n",
    "    def FFN(self,x):\n",
    "        tmp1 = self.act(self.ffn1(x))\n",
    "        tmp2 = self.ffn2(tmp1)\n",
    "        output = self.layerNorm_ffn(x+tmp2)\n",
    "        return output\n",
    "\n",
    "    def forward(self,x1,x2):\n",
    "        '''\n",
    "\n",
    "        :param x1: decoder input: (N,L,D)\n",
    "        :param x2: encoder output: (N,L,D)\n",
    "        :return:   (N,L,D)\n",
    "        '''\n",
    "\n",
    "        x1 = self.SelfAttention(x1)\n",
    "        tmp = self.CrossAttention(x1,x2)\n",
    "        output = self.FFN(tmp)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "naive_decoder_layer = NaiveDecoderLayer(200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.4192, -0.3046, -1.0121,  ...,  1.1269, -0.0948, -0.6938],\n         [ 0.5220,  1.4163,  0.5279,  ..., -1.4432, -0.1654, -0.7548],\n         [-0.9789, -1.1642,  1.0576,  ..., -0.2250, -1.0317, -0.0946],\n         ...,\n         [ 0.9443,  0.9501,  0.3716,  ...,  0.7471,  2.3221,  0.3581],\n         [-1.2199,  0.0381,  2.2465,  ...,  1.8185, -0.5360, -0.1274],\n         [ 0.3285, -1.8043, -1.8896,  ...,  0.9083,  0.7956,  0.6076]],\n\n        [[-0.2989, -0.6812,  0.8181,  ...,  0.2203, -1.0936,  1.0009],\n         [-1.6739, -0.6420, -1.6010,  ...,  1.1678,  0.1683,  0.2897],\n         [ 1.8925,  0.0761, -0.2128,  ...,  1.2759,  0.3530, -0.0767],\n         ...,\n         [ 1.2836,  0.3583, -0.1468,  ..., -0.2630, -1.5336, -1.6907],\n         [ 0.0603, -0.1419, -1.1213,  ...,  1.9134, -0.3812,  0.8961],\n         [-0.1241,  1.7099, -0.3782,  ..., -0.6878,  0.2951, -0.6351]],\n\n        [[-1.2473,  0.0931,  0.2344,  ...,  0.4220, -0.1690,  1.9427],\n         [-0.3613,  2.2734, -0.0558,  ..., -0.3123, -0.5227,  0.4109],\n         [-0.2472,  1.1279, -1.6138,  ...,  0.7245,  0.0656, -1.0517],\n         ...,\n         [-0.9283, -0.3123,  0.6649,  ...,  1.3954,  0.9575, -0.3730],\n         [-0.8998,  0.3399,  0.3433,  ..., -0.5042, -0.3083,  0.2813],\n         [ 1.1026, -2.4245, -0.5212,  ...,  2.0486, -1.6505, -1.2863]],\n\n        ...,\n\n        [[-0.1144, -1.0953,  1.0000,  ...,  0.4391,  0.1617,  0.6469],\n         [-0.0119,  2.1957, -2.0718,  ...,  0.5785,  0.1575, -0.4638],\n         [ 0.4776,  0.7496,  0.0064,  ..., -1.4463, -1.2671,  0.9908],\n         ...,\n         [ 1.4300, -0.6374, -1.3306,  ..., -0.0736, -0.9410, -0.3138],\n         [ 1.7383, -1.5613, -0.1462,  ..., -2.0546, -0.6405, -0.5762],\n         [-0.1935, -0.7129, -0.0611,  ...,  0.4999,  0.0574,  0.6513]],\n\n        [[-1.6364, -1.5174, -0.2086,  ...,  2.6080,  0.0804,  0.1105],\n         [ 0.4106, -0.1735, -0.2379,  ...,  0.4119,  0.0343, -0.3202],\n         [-0.1784,  1.6324,  0.8671,  ...,  0.4978, -1.1875,  0.4501],\n         ...,\n         [ 0.2078, -0.2643, -0.4830,  ...,  0.7342, -0.6148, -1.5086],\n         [-0.2291, -1.3699,  1.1999,  ...,  0.2068,  0.3385,  0.8531],\n         [ 0.1597,  0.6220,  1.3059,  ..., -0.2965, -1.3923,  0.7295]],\n\n        [[-0.2520, -0.5770,  1.8781,  ..., -0.2488, -0.6267, -0.5613],\n         [ 0.7321,  0.7162, -0.1849,  ..., -0.0526,  1.1969,  0.3356],\n         [ 0.1228,  0.0865,  0.6013,  ..., -0.1964,  1.2762, -1.1366],\n         ...,\n         [-0.3829, -0.8296,  0.3134,  ..., -1.4142, -0.2846,  0.0044],\n         [ 0.8517,  0.1428, -0.7914,  ...,  0.8592,  0.8160,  1.4285],\n         [ 2.2097, -1.2544, -1.7994,  ...,  0.1263,  0.2788, -0.5067]]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X1 = np.random.randn(10,50,200)\n",
    "X1 = torch.Tensor(X1)\n",
    "X1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.4205,  1.1311, -1.4933,  ..., -0.1737, -1.1983, -1.3437],\n         [ 0.4508,  0.1779,  0.0160,  ..., -0.4977, -0.0788, -0.3522],\n         [ 0.0479,  0.2017, -2.6437,  ...,  0.1751, -1.4324,  1.1254],\n         ...,\n         [ 0.0070, -0.2205, -0.2593,  ...,  0.5204,  0.3335, -1.8348],\n         [ 1.5172,  1.2976, -0.0281,  ..., -0.4320, -1.3770,  1.5681],\n         [-0.0226,  1.0358,  0.1264,  ..., -0.6032,  0.5050, -0.8105]],\n\n        [[ 0.1050, -0.7592, -0.7361,  ..., -0.6486, -0.8526,  0.7178],\n         [-0.3437,  0.0080,  1.0824,  ...,  0.5045,  0.2328,  0.5675],\n         [-1.9728,  0.7277,  1.2641,  ...,  0.2246,  0.4251,  1.4439],\n         ...,\n         [-1.0379,  0.6301,  0.5887,  ..., -2.6204,  0.1260, -0.8174],\n         [-1.6262,  0.9781, -0.2662,  ...,  0.5593,  0.9278,  0.8802],\n         [-0.6525,  0.3195,  2.3799,  ..., -0.1724,  1.8554, -1.3553]],\n\n        [[ 0.5035,  0.7579, -0.6087,  ...,  1.4124,  1.0154,  0.0637],\n         [-0.4996,  0.1251,  0.1392,  ..., -0.3703, -1.7426, -0.5329],\n         [ 0.1764, -0.6907, -0.0767,  ..., -0.9181,  0.2178, -0.4566],\n         ...,\n         [-1.7522, -0.4232,  0.9445,  ..., -1.6409, -1.0627,  0.1781],\n         [ 0.1792, -2.0200, -0.6313,  ..., -0.7053, -0.4281,  0.4222],\n         [ 0.9762, -0.8401,  1.8634,  ..., -1.2167, -0.1148, -0.7440]],\n\n        ...,\n\n        [[ 1.0583,  1.0043, -1.4787,  ...,  1.4772, -0.2789,  0.0406],\n         [ 1.1129, -0.4732, -0.8249,  ..., -0.7898,  1.3189, -1.6624],\n         [-0.0607,  0.6237,  0.9095,  ...,  0.4728,  0.9705, -0.2580],\n         ...,\n         [ 0.7253,  0.7501, -0.0201,  ..., -0.2205,  0.3175,  0.0874],\n         [-1.3012, -0.5805, -0.5221,  ...,  0.3851,  0.0863,  0.1078],\n         [-0.2913, -1.0064, -0.4113,  ..., -0.1472,  1.4463,  2.0023]],\n\n        [[ 0.1017,  0.1841, -0.3798,  ..., -1.1547, -0.5392, -0.2575],\n         [ 0.1243, -1.3245,  0.1012,  ..., -0.3176,  0.9881,  0.7148],\n         [-0.6633, -0.9162,  0.6320,  ..., -0.8272,  1.6705, -1.4443],\n         ...,\n         [ 0.0039,  0.6769,  0.1526,  ..., -0.1991, -0.1557, -0.4701],\n         [ 0.5154,  0.0719, -0.1058,  ...,  1.1331, -0.9144, -0.6762],\n         [ 0.0677,  0.0138, -0.9636,  ..., -0.3647, -0.6769,  1.7227]],\n\n        [[ 0.4814, -2.0820, -0.6265,  ...,  0.6156, -0.4407,  0.5885],\n         [ 0.6766, -0.3830,  0.9438,  ..., -0.2649,  1.7159,  1.6804],\n         [-0.6672, -1.9667, -0.5551,  ..., -1.9543,  1.1631, -0.2392],\n         ...,\n         [-1.6313,  0.6025,  0.0508,  ...,  0.6715,  1.4010,  0.1646],\n         [-0.9508,  0.6373, -2.1944,  ...,  0.4211, -0.2862, -0.4398],\n         [-0.1642,  0.2958,  1.3069,  ..., -0.0651, -0.2951,  0.6557]]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = np.random.randn(10,50,200)\n",
    "X2 = torch.Tensor(X2)\n",
    "X2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([10, 50, 200]),\n tensor([[[-0.3869, -0.5746, -0.5209,  ...,  1.4008, -0.2333, -1.2964],\n          [ 0.6724,  1.2441,  0.7611,  ..., -1.4689, -0.2661, -1.1754],\n          [-0.9262, -0.9403,  0.9262,  ..., -0.3938, -1.3850, -0.5305],\n          ...,\n          [ 1.3099,  0.8876,  0.6687,  ...,  1.1960,  2.1952,  0.0927],\n          [-1.3134,  0.1008,  2.5786,  ...,  1.7610, -1.0919, -0.2164],\n          [ 0.4003, -2.0306, -1.7047,  ...,  0.8807,  0.7576,  0.1548]],\n \n         [[-0.2834, -0.3894,  0.7972,  ...,  0.7084, -1.2533,  0.8241],\n          [-1.4472, -0.4665, -1.5695,  ...,  1.3613,  0.0084,  0.3219],\n          [ 1.9283,  0.3119,  0.3057,  ...,  2.0656,  0.2988,  0.0633],\n          ...,\n          [ 1.7092,  0.3258, -0.4554,  ..., -0.0654, -1.6422, -2.1645],\n          [ 0.1823, -0.1338, -0.8696,  ...,  2.0757, -0.4466,  0.6769],\n          [-0.1520,  1.9974, -0.3151,  ..., -0.6242,  0.2950, -0.7862]],\n \n         [[-1.3522, -0.1732,  0.2250,  ...,  0.5865, -0.5035,  1.8003],\n          [ 0.0543,  2.7635,  0.0662,  ..., -0.0946, -0.5815,  0.2675],\n          [ 0.0442,  1.1277, -0.9113,  ...,  0.9436,  0.0848, -0.8984],\n          ...,\n          [-0.5828, -0.2956,  0.5624,  ...,  1.7807,  1.2167, -0.1341],\n          [-0.8313,  0.0433,  0.3783,  ..., -0.1135, -0.2511,  0.0947],\n          [ 0.9021, -2.0859, -0.3552,  ...,  1.9911, -1.7329, -1.2238]],\n \n         ...,\n \n         [[-0.2240, -1.2731,  1.1875,  ...,  0.7308,  0.3306,  0.6108],\n          [ 0.0999,  1.5243, -1.6494,  ...,  0.4754, -0.1093, -0.7245],\n          [ 0.3663,  0.6545,  0.3645,  ..., -1.1810, -1.3292,  0.6526],\n          ...,\n          [ 1.3243, -0.8537, -1.1426,  ..., -0.0608, -0.8879, -0.7952],\n          [ 1.5482, -1.5960,  0.1562,  ..., -1.8104, -0.6802, -0.5600],\n          [-0.1450, -1.1367,  0.2208,  ...,  0.7291,  0.2679,  0.5200]],\n \n         [[-1.0058, -1.7344, -0.1273,  ...,  2.8143, -0.1734, -0.1571],\n          [ 0.2183, -0.1606, -0.5231,  ...,  0.1518,  0.1464, -0.1483],\n          [ 0.0157,  1.6084,  1.1398,  ...,  0.2908, -1.3608,  0.1150],\n          ...,\n          [ 0.1018, -0.8195, -0.3937,  ...,  0.7740, -0.8584, -1.7399],\n          [-0.2301, -1.6199,  0.8481,  ...,  0.0619,  0.2132,  0.9037],\n          [-0.2117,  0.0628,  1.3442,  ..., -0.0742, -1.6445,  0.2992]],\n \n         [[-0.3424, -0.7620,  2.3066,  ..., -0.2664, -0.5250, -0.8556],\n          [ 0.9495,  0.5836, -0.2786,  ...,  0.1371,  1.3344,  0.4935],\n          [ 0.4381,  0.1032,  0.5956,  ...,  0.2330,  1.8215, -0.9118],\n          ...,\n          [ 0.3062, -0.9257,  0.4863,  ..., -0.9263, -0.0714,  0.0430],\n          [ 1.0487,  0.0919, -0.6068,  ...,  1.1299,  1.2557,  1.1842],\n          [ 2.3763, -1.1237, -1.8335,  ..., -0.0276,  0.1955, -0.5037]]],\n        grad_fn=<NativeLayerNormBackward0>))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = naive_decoder_layer(X1,X2)\n",
    "\n",
    "output.shape,output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 缺点:\n",
    "1. 没有dropout\n",
    "2. 没有multi-head attention\n",
    "3. 没有attention mask, subsequence mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}